{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5132ba9f-141d-4722-9fa8-603e3c83c016",
   "metadata": {},
   "source": [
    "# Preparing the dataset and split it to 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ff2911-5ae0-45d3-b9a6-377d6012d2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "n_splits = 5\n",
    "stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "fold_count = 1\n",
    "\n",
    "for train_index, test_index in stratified_kfold.split(X, y_filtered):\n",
    "    train_index, test_index = np.array(train_index), np.array(test_index)\n",
    "\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y_filtered.iloc[train_index], y_filtered.iloc[test_index]\n",
    "    \n",
    "    # use arbitrary for each fold name  \n",
    "    train_csv_filename = f'_fold_{fold_count}_train.csv'\n",
    "    test_csv_filename = f'_{fold_count}_test.csv'\n",
    "\n",
    "    train_df = pd.DataFrame(data=np.column_stack((X_train, y_train)), columns=np.append(X.columns.values, 'target'))\n",
    "    test_df = pd.DataFrame(data=np.column_stack((X_test, y_test)), columns=np.append(X.columns.values, 'target'))\n",
    "\n",
    "    train_df.to_csv(train_csv_filename, index=False)\n",
    "    test_df.to_csv(test_csv_filename, index=False)\n",
    "\n",
    "    fold_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4b4dec-25c3-4919-b624-b334175aeb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Use this for each fold in each iteration \n",
    "\n",
    "\n",
    "merged_df_train = pd.read_csv('_fold_1_train.csv')\n",
    "new_tr_X_train = merged_df_train.drop(columns=['target'])\n",
    "new_tr_Y_train = merged_df_train['target'] \n",
    "\n",
    "merged_df_test = pd.read_csv('_1_test.csv')\n",
    "new_tr_X_test = merged_df_test.drop(columns=['target'])\n",
    "new_tr_y_test = merged_df_test['target'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df4ba89-662d-4ea7-a13f-931d4afb6e12",
   "metadata": {},
   "source": [
    "# DeepInsight Transformation Code\n",
    "\n",
    "\n",
    "This cell of code was obtained from the following URL: https://github.com/alok-ai-lab/pyDeepInsight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c81806-c8d8-46bc-bb0e-23937c044e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "A Numpy implementation of the Asymmetric Greedy Search (AGS) algorithm\n",
    "described in 'A heuristic for the time constrained asymmetric linear sum\n",
    "assignment problem'\n",
    "\n",
    "DOI:10.1007/s10878-015-9979-2\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def _initial(benefit_matrix, shuffle: bool = False):\n",
    "    \"\"\"Initialize the assignment solution array. Sequentially assign each\n",
    "    row to an unassigned column with the maximum benefit\n",
    "\n",
    "    Args:\n",
    "        benefit_matrix: a 2d array of benefit values\n",
    "        shuffle: if True, randomly shuffle the order of the rows prior to the\n",
    "            initial assignment. The default value is False to match the\n",
    "            description in the paper\n",
    "\n",
    "    Returns:\n",
    "        a 1d array of row assignments\n",
    "\n",
    "    \"\"\"\n",
    "    bm = benefit_matrix.copy()\n",
    "    assignment = np.empty((bm.shape[0]), dtype=np.int64)\n",
    "    rows = np.arange(bm.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(rows)\n",
    "    for n in rows:\n",
    "        max_idx = np.argmax(bm[n, :])\n",
    "        assignment[n] = max_idx\n",
    "        bm[:, max_idx] = np.NINF\n",
    "\n",
    "    return assignment\n",
    "\n",
    "\n",
    "def _row_swap_cost(benefit_matrix, assignment, row_idx):\n",
    "    \"\"\"Calculate the costs of swapping a column assignments for a given row\n",
    "    with all other rows and return the swap with the greatest benefit\n",
    "\n",
    "    Args:\n",
    "        benefit_matrix: a 2d array of benefit values\n",
    "        assignment: a 1d array of column assignments\n",
    "        row_idx: the row index on which to calculate swap costs\n",
    "\n",
    "    Returns:\n",
    "        a tuple of the best swap row and the associated benefit\n",
    "\n",
    "    \"\"\"\n",
    "    swap_cost = benefit_matrix[row_idx, assignment] + \\\n",
    "        benefit_matrix[:, assignment[row_idx]]\n",
    "    curr_cost = benefit_matrix[row_idx, assignment[row_idx]] + \\\n",
    "        benefit_matrix[np.arange(benefit_matrix.shape[0]), assignment]\n",
    "    cost = swap_cost - curr_cost\n",
    "    cost[row_idx] = np.NINF\n",
    "    best_row = np.argmax(cost)\n",
    "    best_row_benefit = cost[best_row]\n",
    "    return best_row, best_row_benefit\n",
    "\n",
    "\n",
    "def _best_row_swap(benefit_matrix, assignment):\n",
    "    \"\"\"Determine the best row swap for all rows\n",
    "\n",
    "    Args:\n",
    "        benefit_matrix: a 2d array of benefit values\n",
    "        assignment: a 1d array of column assignments\n",
    "\n",
    "    Returns:\n",
    "        a tuple of arrays for best swap row and the associated benefits\n",
    "    \"\"\"\n",
    "    best_row, best_row_benefit = np.stack(\n",
    "        [_row_swap_cost(benefit_matrix, assignment, r) for r in\n",
    "         np.arange(assignment.shape[0])]).T\n",
    "    best_row = best_row.astype(np.int64)\n",
    "    return best_row, best_row_benefit\n",
    "\n",
    "\n",
    "def _col_swap_cost(benefit_matrix, assignment, row_idx):\n",
    "    \"\"\"Calculate the cost of swapping column assignments for a given row to\n",
    "    unassigned columns and return the best column and the associated benefit\n",
    "\n",
    "    benefit_matrix: a 2d array of benefit values\n",
    "        assignment: a 1d array of column assignments\n",
    "        row_idx: the row index on which to calculate swap costs\n",
    "\n",
    "    Returns:\n",
    "        a tuple of the best swap column and the associated benefit\n",
    "    \"\"\"\n",
    "    valid_idx = np.delete(np.arange(benefit_matrix.shape[1]), assignment)\n",
    "    best_col = valid_idx[benefit_matrix[row_idx, valid_idx].argmax()]\n",
    "    best_col_benefit = benefit_matrix[row_idx, best_col]\n",
    "    return best_col, best_col_benefit\n",
    "\n",
    "\n",
    "def _best_col_swap(benefit_matrix, assignment):\n",
    "    \"\"\"Determine the best unassigned column swap for all rows\n",
    "\n",
    "    Args:\n",
    "        benefit_matrix: a 2d array of benefit values\n",
    "        assignment: a 1d array of column assignments\n",
    "\n",
    "    Returns:\n",
    "        a tuple of arrays for best unassigned columns and the associated\n",
    "         benefits\n",
    "    \"\"\"\n",
    "    row_idx = np.arange(assignment.shape[0])\n",
    "    bm_unused = benefit_matrix.copy()\n",
    "    bm_unused[:, assignment] = np.NINF\n",
    "    best_col = np.argmax(bm_unused, axis=1)\n",
    "    best_col_benefit = bm_unused[row_idx, best_col]\n",
    "    return best_col, best_col_benefit\n",
    "\n",
    "\n",
    "def _row_swap(benefit_matrix, assignment, best_row, br_benefit,\n",
    "              best_col, bc_benefit, r_idx):\n",
    "    \"\"\"Swap columns assignments of given row with the best option and then\n",
    "    update swap benefit matrices.\n",
    "\n",
    "    Args:\n",
    "        benefit_matrix: a 2d array of benefit values\n",
    "        assignment: a 1d array of column assignments\n",
    "        best_row: a 1d array of the best row swap for each row\n",
    "        br_benefit: a 1d array of the benefit associated with the best row swap\n",
    "        best_col: a 1d array of the best unassigned column for each row\n",
    "        bc_benefit: a 1d array of the benefit associated with the best column\n",
    "        r_idx: row to swap\n",
    "\n",
    "    Return:\n",
    "        A tuple of updated assignment and benefit/swap matrices\n",
    "    \"\"\"\n",
    "    rs_idx = best_row[r_idx]\n",
    "    # switch assignments\n",
    "    assignment[[r_idx, rs_idx]] = assignment[[rs_idx, r_idx]]\n",
    "    # update row swap matrices\n",
    "    for idx in (r_idx, rs_idx):\n",
    "        new_row, new_benefit = _row_swap_cost(benefit_matrix, assignment, idx)\n",
    "        best_row[idx] = new_row\n",
    "        br_benefit[idx] = new_benefit\n",
    "    # update the column assignment matrices\n",
    "    for idx in (r_idx, rs_idx):\n",
    "        new_row, new_benefit = _col_swap_cost(benefit_matrix, assignment, idx)\n",
    "        best_col[idx] = new_row\n",
    "        bc_benefit[idx] = new_benefit\n",
    "    return assignment, best_row, br_benefit, best_col, bc_benefit\n",
    "\n",
    "\n",
    "def _col_swap(benefit_matrix, assignment, best_row, br_benefit,\n",
    "              best_col, bc_benefit, r_idx):\n",
    "    \"\"\"Swap columns assignment of given row with the best option unassigned\n",
    "        column and then update swap benefit matrices.\n",
    "\n",
    "        Args:\n",
    "            benefit_matrix: a 2d array of benefit values\n",
    "            assignment: a 1d array of column assignments\n",
    "            best_row: a 1d array of the best row swap for each row\n",
    "            br_benefit: a 1d array of the benefit associated with the best\n",
    "            row swap\n",
    "            best_col: a 1d array of the best unassigned column for each row\n",
    "            bc_benefit: a 1d array of the benefit associated with the best\n",
    "                column\n",
    "            r_idx: row to swap\n",
    "\n",
    "        Return:\n",
    "            A tuple of updated assignment and benefit/swap matrices\n",
    "    \"\"\"\n",
    "    assignment[r_idx] = best_col[r_idx]\n",
    "    # update best row (benefit)\n",
    "    new_row, new_benefit = _row_swap_cost(benefit_matrix, assignment, r_idx)\n",
    "    best_row[r_idx] = new_row\n",
    "    br_benefit[r_idx] = new_benefit\n",
    "    # update best column (benefit)\n",
    "    new_row, new_benefit = _col_swap_cost(benefit_matrix, assignment, r_idx)\n",
    "    best_col[r_idx] = new_row\n",
    "    bc_benefit[r_idx] = new_benefit\n",
    "    return assignment, best_row, br_benefit, best_col, bc_benefit\n",
    "\n",
    "\n",
    "def asymmetric_greedy_search(benefit_matrix, shuffle=False, minimize=False):\n",
    "    \"\"\"A python implementation of the algorithm described in 'A heuristic for\n",
    "        the time constrained asymmetric linear sum assignment problem'\n",
    "\n",
    "        Args:\n",
    "            benefit_matrix: a 2d array of benefit or cost values\n",
    "            shuffle: set to True to randomize order of row initialization\n",
    "            minimize: set to True if a cost matrix rather than a benefit\n",
    "                matrix is provided\n",
    "\n",
    "        Returns:\n",
    "            a tuple of row indices and assigned column indices\n",
    "    \"\"\"\n",
    "\n",
    "    bm = benefit_matrix\n",
    "    if minimize:\n",
    "        bm = -benefit_matrix.copy()\n",
    "\n",
    "    assignment = _initial(bm, shuffle=shuffle)\n",
    "    brs, brb = _best_row_swap(bm, assignment)\n",
    "    bcs, bcb = _best_col_swap(bm, assignment)\n",
    "\n",
    "    brb_max = np.amax(brb)\n",
    "    bcb_max = np.amax(bcb)\n",
    "\n",
    "    while brb_max > 0 or bcb_max > 0:\n",
    "        while brb_max > 0 or bcb_max > 0:\n",
    "            if brb_max > bcb_max:\n",
    "                r = np.argmax(brb)\n",
    "                assignment, brs, brb, bcs, bcb = \\\n",
    "                    _row_swap(bm, assignment, brs, brb, bcs, bcb, r)\n",
    "            else:\n",
    "                r = np.argmax(bcb)\n",
    "                assignment, brs, brb, bcs, bcb = \\\n",
    "                    _col_swap(bm, assignment, brs, brb, bcs, bcb, r)\n",
    "            brb_max = np.amax(brb)\n",
    "            bcb_max = np.amax(bcb)\n",
    "        brs, brb = _best_row_swap(bm, assignment)\n",
    "        bcs, bcb = _best_col_swap(bm, assignment)\n",
    "        brb_max = np.amax(brb)\n",
    "        bcb_max = np.amax(bcb)\n",
    "\n",
    "    return np.arange(bm.shape[0]), assignment\n",
    "\n",
    "\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "\n",
    "class Norm2Scaler:\n",
    "    \"\"\"Log normalize and scale data\n",
    "\n",
    "    Log normalization and scaling procedure as described as norm-2 in the\n",
    "    DeepInsight paper supplementary information.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: Optional[np.ndarray] = None):\n",
    "        self._min0 = X.min(axis=0)\n",
    "        self._max = np.log(X + np.abs(self._min0) + 1).max()\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X: np.ndarray, y: Optional[np.ndarray] = None\n",
    "                      ) -> np.ndarray:\n",
    "        self._min0 = X.min(axis=0)\n",
    "        X_norm = np.log(X + np.abs(self._min0) + 1)\n",
    "        self._max = X_norm.max()\n",
    "        return X_norm / self._max\n",
    "\n",
    "    def transform(self, X: np.ndarray, y: Optional[np.ndarray] = None\n",
    "                  ) -> np.ndarray:\n",
    "        X_norm = np.log(X + np.abs(self._min0) + 1).clip(0, None)\n",
    "        return (X_norm / self._max).clip(0, 1)\n",
    "\n",
    "    \n",
    "from typing import Sequence\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data.sampler import Sampler, RandomSampler\n",
    "\n",
    "\n",
    "class StratifiedEventBatchSampler(Sampler):\n",
    "    \"\"\"Samples elements with from a set with binary labelling to ensure\n",
    "    the event label (1) is evenly distributed across batches.\n",
    "\n",
    "    This sampler is useful when the loss function requires at least one event,\n",
    "    such as in the case of a Cox Proportional Hazard based loss.\n",
    "    \"\"\"\n",
    "\n",
    "    events: torch.Tensor\n",
    "    batch_size: int\n",
    "\n",
    "    def __init__(self, events: Sequence[int], batch_size: int) -> None:\n",
    "        \"\"\"Generate an StratifiedBinaryBatchSampler instance\n",
    "\n",
    "        Args:\n",
    "            events: int sequence of binary event labels (0, 1)\n",
    "            batch_size: int that defines size of mini-batch.\n",
    "        \"\"\"\n",
    "        if not isinstance(batch_size, int) or batch_size <= 0:\n",
    "            raise ValueError(\"batch_size should be a positive integer value, \"\n",
    "                             \"but got batch_size={}\".format(batch_size))\n",
    "        self.events = torch.as_tensor(events, dtype=torch.int64)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.events0_idx = torch.where(self.events == 0)[0]\n",
    "        self.events1_idx = torch.where(self.events == 1)[0]\n",
    "\n",
    "        self._len = math.ceil(len(self.events) / self.batch_size)\n",
    "\n",
    "        if self.events1_idx.shape[0] < self._len:\n",
    "            raise ValueError(\"the number of events ({}) must be equal or \"\n",
    "                             \"larger than the number of batches ({})\"\n",
    "                             .format(self.events1_idx.shape[0], self._len))\n",
    "\n",
    "        # Get batch sizes for each\n",
    "        self.batch0_size = math.ceil(\n",
    "            self.events0_idx.shape[0] / self.events.shape[0] * batch_size)\n",
    "        self.batch1_size = math.floor(\n",
    "            self.events1_idx.shape[0] / self.events.shape[0] * batch_size)\n",
    "\n",
    "        self.sampler0 = RandomSampler(self.events0_idx, replacement=False)\n",
    "        self.sampler1 = RandomSampler(self.events1_idx, replacement=False)\n",
    "\n",
    "    def __iter0__(self):\n",
    "        \"\"\"Iterate the non-event (0) label sampler\"\"\"\n",
    "        batch = torch.tensor([], dtype=torch.int64)\n",
    "        for idx in self.sampler0:\n",
    "            idx0 = self.events0_idx[idx, None]\n",
    "            batch = torch.cat((batch, idx0), 0)\n",
    "            if batch.shape[0] == self.batch0_size:\n",
    "                yield batch\n",
    "                batch = torch.tensor([], dtype=torch.int64)\n",
    "        if batch.shape[0] > 0:\n",
    "            yield batch\n",
    "\n",
    "    def __iter1__(self):\n",
    "        \"\"\"Iterate the event (0) label sampler\"\"\"\n",
    "        batch = torch.tensor([], dtype=torch.int64)\n",
    "        for idx in self.sampler1:\n",
    "            idx1 = self.events1_idx[idx, None]\n",
    "            batch = torch.cat((batch, idx1), 0)\n",
    "            if batch.shape[0] == self.batch1_size:\n",
    "                yield batch\n",
    "                batch = torch.tensor([], dtype=torch.int64)\n",
    "        if batch.shape[0] > 0:\n",
    "            yield batch\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Generate the indices for the next batch of elements\"\"\"\n",
    "        for batch0, batch1 in zip(self.__iter0__(), self.__iter1__()):\n",
    "            batch = torch.cat((batch0, batch1), 0).sort()[0]\n",
    "            yield batch\n",
    "\n",
    "    # Removed - https://github.com/Lightning-AI/lightning/issues/2429\n",
    "    # def __len__(self):\n",
    "    #     \"\"\"Return the number of batches\"\"\"\n",
    "    #     return self._len\n",
    "\n",
    "    \n",
    "from typing import Union, Any, Optional, Tuple\n",
    "from typing_extensions import Protocol\n",
    "from numpy.typing import ArrayLike\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from matplotlib import pyplot as plt\n",
    "import inspect\n",
    "\n",
    "# from .utils import asymmetric_greedy_search\n",
    "\n",
    "class ManifoldLearner(Protocol):\n",
    "    def fit_transform(self: 'ManifoldLearner',\n",
    "                      X: np.ndarray) -> np.ndarray: pass\n",
    "\n",
    "\n",
    "class ImageTransformer:\n",
    "    \"\"\"Transform features to an image matrix using dimensionality reduction\n",
    "\n",
    "    This class takes in data normalized between 0 and 1 and converts it to a\n",
    "    CNN compatible 'image' matrix\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_extractor: Union[str, ManifoldLearner] = 'tsne',\n",
    "                 discretization: str = 'bin',\n",
    "                 pixels: Union[int, Tuple[int, int]] = (224, 224)) -> None:\n",
    "        \"\"\"Generate an ImageTransformer instance\n",
    "\n",
    "        Args:\n",
    "            feature_extractor: string of value ('tsne', 'pca', 'kpca') or a\n",
    "                class instance with method `fit_transform` that returns a\n",
    "                2-dimensional array of extracted features.\n",
    "            discretization: string of values ('bin', 'assignment'). Defines\n",
    "                the method for discretizing dimensionally reduced data to pixel\n",
    "                coordinates.\n",
    "            pixels: int (square matrix) or tuple of ints (height, width) that\n",
    "                defines the size of the image matrix.\n",
    "        \"\"\"\n",
    "        self._fe = self._parse_feature_extractor(feature_extractor)\n",
    "        self._dm = self._parse_discretization(discretization)\n",
    "        self._pixels = self._parse_pixels(pixels)\n",
    "        self._xrot = np.empty(0)\n",
    "        self._coords = np.empty(0)\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_pixels(pixels: Union[int, Tuple[int, int]]) -> Tuple[int, int]:\n",
    "        \"\"\"Check and correct pixel parameter\n",
    "\n",
    "        Args:\n",
    "            pixels: int (square matrix) or tuple of ints (height, width) that\n",
    "                defines the size of the image matrix.\n",
    "        \"\"\"\n",
    "        if isinstance(pixels, int):\n",
    "            pixels = (pixels, pixels)\n",
    "        return pixels\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_feature_extractor(\n",
    "            feature_extractor: Union[str, ManifoldLearner]) -> ManifoldLearner:\n",
    "        \"\"\"Validate the feature extractor value passed to the\n",
    "        constructor method and return correct method\n",
    "\n",
    "        Args:\n",
    "            feature_extractor: string of value ('tsne', 'pca', 'kpca') or a\n",
    "                class instance with method `fit_transform` that returns a\n",
    "                2-dimensional array of extracted features.\n",
    "\n",
    "        Returns:\n",
    "            function\n",
    "        \"\"\"\n",
    "        if isinstance(feature_extractor, str):\n",
    "            fe = feature_extractor.casefold()\n",
    "            if fe == 'tsne'.casefold():\n",
    "                fe_func = TSNE(n_components=2, metric='cosine')\n",
    "            elif fe == 'pca'.casefold():\n",
    "                fe_func = PCA(n_components=2)\n",
    "            elif fe == 'kpca'.casefold():\n",
    "                fe_func = KernelPCA(n_components=2, kernel='rbf')\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"feature_extractor '{feature_extractor}' not valid\")\n",
    "        elif hasattr(feature_extractor, 'fit_transform') and \\\n",
    "                inspect.ismethod(feature_extractor.fit_transform):\n",
    "            fe_func = feature_extractor\n",
    "        else:\n",
    "            raise TypeError('Parameter feature_extractor is not a '\n",
    "                            'string nor has method \"fit_transform\"')\n",
    "        return fe_func\n",
    "\n",
    "    @classmethod\n",
    "    def _parse_discretization(cls, method: str):\n",
    "        \"\"\"Validate the discretization value passed to the\n",
    "        constructor method and return correct function\n",
    "\n",
    "        Args:\n",
    "            method: string of value ('bin', 'assignment')\n",
    "\n",
    "        Returns:\n",
    "            function\n",
    "        \"\"\"\n",
    "        if method == 'bin':\n",
    "            func = cls.coordinate_binning\n",
    "        elif method == 'assignment' or method == 'lsa':\n",
    "            func = cls.coordinate_optimal_assignment\n",
    "        elif method == 'ags':\n",
    "            func = cls.coordinate_heuristic_assignment\n",
    "        else:\n",
    "            raise ValueError(f\"discretization method '{method}' not valid\")\n",
    "        return func\n",
    "\n",
    "    @classmethod\n",
    "    def coordinate_binning(cls, position: np.ndarray,\n",
    "                           px_size: Tuple[int, int]) -> np.ndarray:\n",
    "        \"\"\"Determine the pixel locations of each feature based on the overlap of\n",
    "        feature position and pixel locations.\n",
    "\n",
    "        Args:\n",
    "            position: a 2d array of feature coordinates\n",
    "            px_size: tuple with image dimensions\n",
    "\n",
    "        Returns:\n",
    "            a 2d array of feature to pixel mappings\n",
    "        \"\"\"\n",
    "        scaled = cls.scale_coordinates(position, px_size)\n",
    "        px_binned = np.floor(scaled).astype(int)\n",
    "        # Need to move maximum values into the lower bin\n",
    "        px_binned[:, 0][px_binned[:, 0] == px_size[0]] = px_size[0] - 1\n",
    "        px_binned[:, 1][px_binned[:, 1] == px_size[1]] = px_size[1] - 1\n",
    "        return px_binned\n",
    "\n",
    "    @staticmethod\n",
    "    def lsap_optimal_solution(cost_matrix):\n",
    "        return linear_sum_assignment(cost_matrix)\n",
    "\n",
    "    @staticmethod\n",
    "    def lsap_heuristic_solution(cost_matrix):\n",
    "        return asymmetric_greedy_search(cost_matrix,\n",
    "                                        shuffle=True,\n",
    "                                        minimize=True)\n",
    "\n",
    "    @classmethod\n",
    "    def coordinate_optimal_assignment(cls, position: np.ndarray,\n",
    "                                      px_size: Tuple[int, int]) -> np.ndarray:\n",
    "        \"\"\"Determine the pixel location of each feature using a linear sum\n",
    "        assignment problem solution on the exponential on the euclidean\n",
    "        distances between the features and the pixels\n",
    "\n",
    "        Args:\n",
    "            position: a 2d array of feature coordinates\n",
    "            px_size: tuple with image dimensions\n",
    "\n",
    "        Returns:\n",
    "            a 2d array of feature to pixel mappings\n",
    "        \"\"\"\n",
    "        scaled = cls.scale_coordinates(position, px_size)\n",
    "        px_centers = cls.calculate_pixel_centroids(px_size)\n",
    "\n",
    "        # calculate distances\n",
    "        k = np.prod(px_size)\n",
    "        clustered = scaled.shape[0] > k\n",
    "        if clustered:\n",
    "            kmeans = KMeans(n_clusters=k).fit(scaled)\n",
    "            cl_labels = kmeans.labels_\n",
    "            cl_centers = kmeans.cluster_centers_\n",
    "            dist = cdist(cl_centers, px_centers, metric='euclidean')\n",
    "        else:\n",
    "            dist = cdist(scaled, px_centers, metric='euclidean')\n",
    "        # assignment of features/clusters to pixels\n",
    "        lsa = cls.lsap_optimal_solution(dist)\n",
    "        px_assigned = np.empty(scaled.shape, dtype=int)\n",
    "        for i in range(scaled.shape[0]):\n",
    "            if clustered:\n",
    "                # The feature at i\n",
    "                # Is mapped to the cluster j=clabl[i]\n",
    "                # Which is mapped to the pixel center clust_cntr[j]\n",
    "                # Which is mapped to the pixel k = lsa[1][j]\n",
    "                # For pixel k, x = k % px_size[0] and y = k // px_size[0]\n",
    "                j = cl_labels[i]\n",
    "            else:\n",
    "                j = i\n",
    "            ki = lsa[1][j]\n",
    "            xi = ki % px_size[0]\n",
    "            yi = ki // px_size[0]\n",
    "            px_assigned[i] = [yi, xi]\n",
    "        return px_assigned\n",
    "\n",
    "    @classmethod\n",
    "    def coordinate_heuristic_assignment(cls, position: np.ndarray,\n",
    "                                        px_size: Tuple[int, int]) -> np.ndarray:\n",
    "\n",
    "        scaled = cls.scale_coordinates(position, px_size)\n",
    "        px_centers = cls.calculate_pixel_centroids(px_size)\n",
    "\n",
    "        # calculate distances\n",
    "        # AGS requires asymmetric assignment so k must be less than pixels\n",
    "        k = np.prod(px_size) - 1\n",
    "        clustered = scaled.shape[0] > k\n",
    "        if clustered:\n",
    "            kmeans = KMeans(n_clusters=k).fit(scaled)\n",
    "            cl_labels = kmeans.labels_\n",
    "            cl_centers = kmeans.cluster_centers_\n",
    "            dist = cdist(cl_centers, px_centers, metric='euclidean')\n",
    "        else:\n",
    "            dist = cdist(scaled, px_centers, metric='euclidean')\n",
    "        # assignment of features/clusters to pixels\n",
    "        lsa = cls.lsap_heuristic_solution(dist)\n",
    "        px_assigned = np.empty(scaled.shape, dtype=int)\n",
    "        for i in range(scaled.shape[0]):\n",
    "            if clustered:\n",
    "                j = cl_labels[i]\n",
    "            else:\n",
    "                j = i\n",
    "            ki = lsa[1][j]\n",
    "            xi = ki % px_size[0]\n",
    "            yi = ki // px_size[0]\n",
    "            px_assigned[i] = [yi, xi]\n",
    "        return px_assigned\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_pixel_centroids(px_size: Tuple[int, int]) -> np.ndarray:\n",
    "        \"\"\"Generate a 2d array of the centroid of each pixel\n",
    "\n",
    "        Args:\n",
    "            px_size: tuple with image dimensions\n",
    "\n",
    "        Returns:\n",
    "            a 2d array of pixel centroid locations\n",
    "        \"\"\"\n",
    "        px_map = np.empty((np.prod(px_size), 2))\n",
    "        for i in range(0, px_size[0]):\n",
    "            for j in range(0, px_size[1]):\n",
    "                px_map[i * px_size[0] + j] = [i, j]\n",
    "        px_centroid = px_map + 0.5\n",
    "        return px_centroid\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: Optional[ArrayLike] = None,\n",
    "            plot: bool = False):\n",
    "        \"\"\"Train the image transformer from the training set (X)\n",
    "\n",
    "        Args:\n",
    "            X: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            y: Ignored. Present for continuity with scikit-learn\n",
    "            plot: boolean of whether to produce a scatter plot showing the\n",
    "                feature reduction, hull points, and minimum bounding rectangle\n",
    "\n",
    "        Returns:\n",
    "            self: object\n",
    "        \"\"\"\n",
    "        # perform dimensionality reduction\n",
    "        x_new = self._fe.fit_transform(X.T)\n",
    "        # get the convex hull for the points\n",
    "        chvertices = ConvexHull(x_new).vertices\n",
    "        hull_points = x_new[chvertices]\n",
    "        # determine the minimum bounding rectangle\n",
    "        mbr, mbr_rot = self._minimum_bounding_rectangle(hull_points)\n",
    "        # rotate the matrix\n",
    "        # save the rotated matrix in case user wants to change the pixel size\n",
    "        self._xrot = np.dot(mbr_rot, x_new.T).T\n",
    "        # determine feature coordinates based on pixel dimension\n",
    "        self._calculate_coords()\n",
    "        # plot rotation diagram if requested\n",
    "        if plot is True:\n",
    "            plt.scatter(x_new[:, 0], x_new[:, 1], s=1,\n",
    "                        cmap=plt.cm.get_cmap(\"jet\", 10), alpha=0.2)\n",
    "            plt.fill(x_new[chvertices, 0], x_new[chvertices, 1],\n",
    "                     edgecolor='r', fill=False)\n",
    "            plt.fill(mbr[:, 0], mbr[:, 1], edgecolor='g', fill=False)\n",
    "            plt.gca().set_aspect('equal', adjustable='box')\n",
    "            plt.show()\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def pixels(self) -> Tuple[int, int]:\n",
    "        \"\"\"The image matrix dimensions\n",
    "\n",
    "        Returns:\n",
    "            tuple: the image matrix dimensions (height, width)\n",
    "\n",
    "        \"\"\"\n",
    "        return self._pixels\n",
    "\n",
    "    @pixels.setter\n",
    "    def pixels(self, pixels: Union[int, Tuple[int, int]]) -> None:\n",
    "        \"\"\"Set the image matrix dimension\n",
    "\n",
    "        Args:\n",
    "            pixels: int or tuple with the dimensions (height, width)\n",
    "            of the image matrix\n",
    "\n",
    "        \"\"\"\n",
    "        if isinstance(pixels, int):\n",
    "            pixels = (pixels, pixels)\n",
    "        self._pixels = pixels\n",
    "        # recalculate coordinates if already fit\n",
    "        if hasattr(self, '_coords'):\n",
    "            self._calculate_coords()\n",
    "\n",
    "    @staticmethod\n",
    "    def scale_coordinates(coords: np.ndarray, dim_max: ArrayLike) -> np.ndarray:\n",
    "        \"\"\"Transforms a list of n-dimensional coordinates by scaling them\n",
    "        between zero and the given dimensional maximum\n",
    "\n",
    "        Args:\n",
    "            coords: a 2d ndarray of coordinates\n",
    "            dim_max: a list of maximum ranges for each dimension of coords\n",
    "\n",
    "        Returns:\n",
    "            a 2d ndarray of scaled coordinates\n",
    "        \"\"\"\n",
    "        data_min = coords.min(axis=0)\n",
    "        data_max = coords.max(axis=0)\n",
    "        std = (coords - data_min) / (data_max - data_min)\n",
    "        scaled = np.multiply(std, dim_max)\n",
    "        return scaled\n",
    "\n",
    "    def _calculate_coords(self) -> None:\n",
    "        \"\"\"Calculate the matrix coordinates of each feature based on the\n",
    "        pixel dimensions.\n",
    "        \"\"\"\n",
    "        scaled = self.scale_coordinates(self._xrot, self._pixels)\n",
    "        px_coords = self._dm(scaled, self._pixels)\n",
    "        self._coords = px_coords\n",
    "\n",
    "    def transform(self, X: np.ndarray, img_format: str = 'rgb',\n",
    "                  empty_value: int = 0) -> np.ndarray:\n",
    "        \"\"\"Transform the input matrix into image matrices\n",
    "\n",
    "        Args:\n",
    "            X: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "                where n_features matches the training set.\n",
    "            img_format: The format of the image matrix to return.\n",
    "                'scalar' returns an array of shape (M, N). 'rgb' returns\n",
    "                a numpy.ndarray of shape (M, N, 3) that is compatible with PIL.\n",
    "            empty_value: numeric value to fill elements where no features are\n",
    "                mapped. Default = 0.\n",
    "\n",
    "        Returns:\n",
    "            A list of n_samples numpy matrices of dimensions set by\n",
    "            the pixel parameter\n",
    "        \"\"\"\n",
    "        img_coords = pd.DataFrame(np.vstack((\n",
    "            self._coords.T,\n",
    "            X\n",
    "        )).T).groupby([0, 1], as_index=False).mean()\n",
    "\n",
    "        img_list = []\n",
    "        blank_mat = np.zeros(self._pixels)\n",
    "        if empty_value != 0:\n",
    "            blank_mat[:] = empty_value\n",
    "        for z in range(2, img_coords.shape[1]):\n",
    "            img_matrix = blank_mat.copy()\n",
    "            img_matrix[img_coords[0].astype(int),\n",
    "                       img_coords[1].astype(int)] = img_coords[z]\n",
    "            img_list.append(img_matrix)\n",
    "\n",
    "        # img_matrices = np.empty(0) ---- REMOVE?\n",
    "        if img_format == 'rgb':\n",
    "            img_matrices = np.array([self._mat_to_rgb(m) for m in img_list])\n",
    "        elif img_format == 'scalar':\n",
    "            img_matrices = np.stack(img_list)\n",
    "        else:\n",
    "            raise ValueError(f\"'{img_format}' not accepted for img_format\")\n",
    "\n",
    "        return img_matrices\n",
    "\n",
    "    def fit_transform(self, X: np.ndarray, **kwargs: Any) -> np.ndarray:\n",
    "        \"\"\"Train the image transformer from the training set (X) and return\n",
    "        the transformed data.\n",
    "\n",
    "        Args:\n",
    "            X: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "\n",
    "        Returns:\n",
    "            A list of n_samples numpy matrices of dimensions set by\n",
    "            the pixel parameter\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X, **kwargs)\n",
    "\n",
    "    def inverse_transform(self, img: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Transform an image layer back to its original space.\n",
    "            Args:\n",
    "                img:\n",
    "\n",
    "            Returns:\n",
    "                A list of n_samples numpy matrices of dimensions set by\n",
    "                the pixel parameter\n",
    "        \"\"\"\n",
    "        if img.ndim == 2 and img.shape == self._pixels:\n",
    "            X = img[self._coords[:, 0], self._coords[:, 1]]\n",
    "        elif img.ndim == 3 and img.shape[-2:] == self._pixels:\n",
    "            X = img[:, self._coords[:, 0], self._coords[:, 1]]\n",
    "        elif img.ndim == 3 and img.shape[0:2] == self._pixels:\n",
    "            X = img[self._coords[:, 0], self._coords[:, 1], :]\n",
    "        elif img.ndim == 4 and img.shape[1:3] == self._pixels:\n",
    "            X = img[:, self._coords[:, 0], self._coords[:, 1], :]\n",
    "        else:\n",
    "            raise ValueError((f\"Expected dimensions of (B, {self._pixels[0]}, \"\n",
    "                              f\"{self._pixels[1]}, C) where B and C are \"\n",
    "                              f\"optional, but got {img.shape}\"))\n",
    "        return X\n",
    "\n",
    "    def feature_density_matrix(self) -> np.ndarray:\n",
    "        \"\"\"Generate image matrix with feature counts per pixel\n",
    "\n",
    "        Returns:\n",
    "            img_matrix (ndarray): matrix with feature counts per pixel\n",
    "        \"\"\"\n",
    "        fdmat = np.zeros(self._pixels)\n",
    "        np.add.at(fdmat, tuple(self._coords.T), 1)\n",
    "        return fdmat\n",
    "\n",
    "    def coords(self) -> np.ndarray:\n",
    "        \"\"\"Get feature coordinates\n",
    "\n",
    "        Returns:\n",
    "            ndarray: the pixel coordinates for features\n",
    "        \"\"\"\n",
    "        return self._coords.copy()\n",
    "\n",
    "    @staticmethod\n",
    "    def _minimum_bounding_rectangle(hull_points: np.ndarray\n",
    "                                    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Find the smallest bounding rectangle for a set of points.\n",
    "\n",
    "        Modified from JesseBuesking at https://stackoverflow.com/a/33619018\n",
    "        Returns a set of points representing the corners of the bounding box.\n",
    "\n",
    "        Args:\n",
    "            hull_points : an nx2 matrix of hull coordinates\n",
    "\n",
    "        Returns:\n",
    "            (tuple): tuple containing\n",
    "                coords (ndarray): coordinates of the corners of the rectangle\n",
    "                rotmat (ndarray): rotation matrix to align edges of rectangle\n",
    "                    to x and y\n",
    "        \"\"\"\n",
    "\n",
    "        pi2 = np.pi / 2\n",
    "        # calculate edge angles\n",
    "        edges = hull_points[1:] - hull_points[:-1]\n",
    "        angles = np.arctan2(edges[:, 1], edges[:, 0])\n",
    "        angles = np.abs(np.mod(angles, pi2))\n",
    "        angles = np.unique(angles)\n",
    "        # find rotation matrices\n",
    "        rotations = np.vstack([\n",
    "            np.cos(angles),\n",
    "            -np.sin(angles),\n",
    "            np.sin(angles),\n",
    "            np.cos(angles)]).T\n",
    "        rotations = rotations.reshape((-1, 2, 2))\n",
    "        # apply rotations to the hull\n",
    "        rot_points = np.dot(rotations, hull_points.T)\n",
    "        # find the bounding points\n",
    "        min_x = np.nanmin(rot_points[:, 0], axis=1)\n",
    "        max_x = np.nanmax(rot_points[:, 0], axis=1)\n",
    "        min_y = np.nanmin(rot_points[:, 1], axis=1)\n",
    "        max_y = np.nanmax(rot_points[:, 1], axis=1)\n",
    "        # find the box with the best area\n",
    "        areas = (max_x - min_x) * (max_y - min_y)\n",
    "        best_idx = np.argmin(areas)\n",
    "        # return the best box\n",
    "        x1 = max_x[best_idx]\n",
    "        x2 = min_x[best_idx]\n",
    "        y1 = max_y[best_idx]\n",
    "        y2 = min_y[best_idx]\n",
    "        rotmat = rotations[best_idx]\n",
    "        # generate coordinates\n",
    "        coords = np.zeros((4, 2))\n",
    "        coords[0] = np.dot([x1, y2], rotmat)\n",
    "        coords[1] = np.dot([x2, y2], rotmat)\n",
    "        coords[2] = np.dot([x2, y1], rotmat)\n",
    "        coords[3] = np.dot([x1, y1], rotmat)\n",
    "\n",
    "        return coords, rotmat\n",
    "\n",
    "    @staticmethod\n",
    "    def _mat_to_rgb(mat: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Convert image matrix to numpy rgb format\n",
    "\n",
    "        Args:\n",
    "            mat: {array-like} (M, N)\n",
    "\n",
    "        Returns:\n",
    "            An numpy.ndarray (M, N, 3) with original values repeated across\n",
    "            RGB channels.\n",
    "        \"\"\"\n",
    "        return np.repeat(mat[:, :, np.newaxis], 3, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacc65ea-5d1b-4131-a757-c03f0c202034",
   "metadata": {},
   "source": [
    "# Normalizing data and Transoformation to images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c34d4f-0e1b-4852-8041-31766625b723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "normalized_data = scaler.fit_transform(new_tr_X_train)\n",
    "\n",
    "normalized_df = pd.DataFrame(normalized_data, columns=new_tr_X_train.columns)\n",
    "\n",
    "normalized_X_train = normalized_df\n",
    "\n",
    "normalized_X_train= normalized_X_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bec7f27-a280-47dd-a447-5d2ec60419ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMGTR = ImageTransformer(feature_extractor='tsne',\n",
    "discretization='assignment', pixels=(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd22ff4-be91-4051-a00d-09606a22a26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset = IMGTR.fit_transform(normalized_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6786aa85-06d9-4167-9922-ba0cc75205ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data_test = scaler.transform(new_tr_X_test)\n",
    "normalized_X_test = normalized_data_test\n",
    "\n",
    "\n",
    "transformed_test_dataset = IMGTR.transform(normalized_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c0bff7-52b0-47c4-8a64-56d7972b811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset_train = transformed_dataset[:, :, :, 0:1]\n",
    "new_dataset_test = transformed_test_dataset[:, :, :, 0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071704d5-fc84-4311-a685-ff53610ccbdb",
   "metadata": {},
   "source": [
    "# cGAN Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee7f7cc-93cd-4e7c-b316-52cc3d61c6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two hidden layers\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def build_generator():\n",
    "    input_noise = Input(shape=(100,))\n",
    "    input_condition = Input(shape=(1,))\n",
    "    merged_input = Concatenate()([input_noise, input_condition])\n",
    "    hidden_layer = Dense(512)(merged_input)\n",
    "    hidden_layer = LeakyReLU(alpha=0.2)(hidden_layer)\n",
    "    hidden_layer = Dense(512)(hidden_layer)\n",
    "    hidden_layer = LeakyReLU(alpha=0.2)(hidden_layer)\n",
    "    generated_output = Dense(784, activation='sigmoid')(hidden_layer)\n",
    "    generated_output = Reshape((28, 28, 1))(generated_output)\n",
    "    model = Model(inputs=[input_noise, input_condition], outputs=generated_output)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def build_discriminator():\n",
    "    input_image = Input(shape=(28, 28, 1,))\n",
    "    input_condition = Input(shape=(1,))\n",
    "    flattened_image = Flatten()(input_image)\n",
    "    merged_input = Concatenate()([flattened_image, input_condition])\n",
    "    hidden_layer = Dense(512)(merged_input)\n",
    "    hidden_layer = LeakyReLU(alpha=0.2)(hidden_layer)\n",
    "    hidden_layer = Dense(512)(hidden_layer)\n",
    "    hidden_layer = LeakyReLU(alpha=0.2)(hidden_layer)\n",
    "    validity = Dense(1, activation='sigmoid')(hidden_layer)\n",
    "    model = Model(inputs=[input_image, input_condition], outputs=validity)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
    "\n",
    "generator = build_generator()\n",
    "\n",
    "noise = Input(shape=(100,))\n",
    "condition = Input(shape=(1,))\n",
    "generated_image = generator([noise, condition])\n",
    "\n",
    "discriminator.trainable = False\n",
    "validity = discriminator([generated_image, condition])\n",
    "\n",
    "combined_model = Model(inputs=[noise, condition], outputs=validity)\n",
    "combined_model.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f408c49b-c919-4728-92c5-6ca5f709dd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# three hidden layers\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def build_generator():\n",
    "    input_noise = Input(shape=(100,))\n",
    "    input_condition = Input(shape=(1,))\n",
    "    merged_input = Concatenate()([input_noise, input_condition])\n",
    "    hidden_layer = Dense(512)(merged_input)\n",
    "    hidden_layer = LeakyReLU(alpha=0.2)(hidden_layer)\n",
    "    hidden_layer = Dense(512)(hidden_layer)\n",
    "    hidden_layer = LeakyReLU(alpha=0.2)(hidden_layer)\n",
    "    hidden_layer = Dense(512)(hidden_layer) \n",
    "    hidden_layer = LeakyReLU(alpha=0.2)(hidden_layer)\n",
    "    generated_output = Dense(784, activation='sigmoid')(hidden_layer)\n",
    "    generated_output = Reshape((28, 28, 1))(generated_output)\n",
    "    model = Model(inputs=[input_noise, input_condition], outputs=generated_output)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def build_discriminator():\n",
    "    input_image = Input(shape=(28, 28, 1,))\n",
    "    input_condition = Input(shape=(1,))\n",
    "    flattened_image = Flatten()(input_image)\n",
    "    merged_input = Concatenate()([flattened_image, input_condition])\n",
    "    hidden_layer = Dense(512)(merged_input)\n",
    "    hidden_layer = LeakyReLU(alpha=0.2)(hidden_layer)\n",
    "    hidden_layer = Dense(512)(hidden_layer)\n",
    "    hidden_layer = LeakyReLU(alpha=0.2)(hidden_layer)\n",
    "    hidden_layer = Dense(512)(hidden_layer)\n",
    "    hidden_layer = LeakyReLU(alpha=0.2)(hidden_layer)\n",
    "    validity = Dense(1, activation='sigmoid')(hidden_layer)\n",
    "    model = Model(inputs=[input_image, input_condition], outputs=validity)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
    "\n",
    "generator = build_generator()\n",
    "\n",
    "noise = Input(shape=(100,))\n",
    "condition = Input(shape=(1,))\n",
    "generated_image = generator([noise, condition])\n",
    "\n",
    "discriminator.trainable = False\n",
    "validity = discriminator([generated_image, condition])\n",
    "\n",
    "combined_model = Model(inputs=[noise, condition], outputs=validity)\n",
    "combined_model.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710f08d4-6f62-4c92-b24e-261df85bbd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Four hidden layers\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def build_generator():\n",
    "    input_noise = Input(shape=(100,))\n",
    "    input_condition = Input(shape=(1,))\n",
    "    merged_input = Concatenate()([input_noise, input_condition])\n",
    "    hidden_layer = Dense(512)(merged_input)\n",
    "    hidden_layer = LeakyReLU(alpha=0.2)(hidden_layer)\n",
    "    hidden_layer = Dense(512)(hidden_layer)\n",
    "    hidden_layer = LeakyReLU(alpha=0.2)(hidden_layer)\n",
    "    hidden_layer = Dense(512)(hidden_layer) \n",
    "    hidden_layer = LeakyReLU(alpha=0.2)(hidden_layer)\n",
    "    hidden_layer = Dense(512)(hidden_layer) \n",
    "    hidden_layer = LeakyReLU(alpha=0.2)(hidden_layer)\n",
    "    generated_output = Dense(784, activation='sigmoid')(hidden_layer)\n",
    "    generated_output = Reshape((28, 28, 1))(generated_output)\n",
    "    model = Model(inputs=[input_noise, input_condition], outputs=generated_output)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def build_discriminator():\n",
    "    input_image = Input(shape=(28, 28, 1,))\n",
    "    input_condition = Input(shape=(1,))\n",
    "    flattened_image = Flatten()(input_image)\n",
    "    merged_input = Concatenate()([flattened_image, input_condition])\n",
    "    hidden_layer = Dense(512)(merged_input)\n",
    "    hidden_layer = LeakyReLU(alpha=0.2)(hidden_layer)\n",
    "    hidden_layer = Dense(512)(hidden_layer)\n",
    "    hidden_layer = LeakyReLU(alpha=0.2)(hidden_layer)\n",
    "    hidden_layer = Dense(512)(hidden_layer)\n",
    "    hidden_layer = LeakyReLU(alpha=0.2)(hidden_layer)\n",
    "    hidden_layer = Dense(512)(hidden_layer)\n",
    "    hidden_layer = LeakyReLU(alpha=0.2)(hidden_layer)\n",
    "    validity = Dense(1, activation='sigmoid')(hidden_layer)\n",
    "    model = Model(inputs=[input_image, input_condition], outputs=validity)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
    "\n",
    "generator = build_generator()\n",
    "\n",
    "noise = Input(shape=(100,))\n",
    "condition = Input(shape=(1,))\n",
    "generated_image = generator([noise, condition])\n",
    "\n",
    "discriminator.trainable = False\n",
    "validity = discriminator([generated_image, condition])\n",
    "\n",
    "combined_model = Model(inputs=[noise, condition], outputs=validity)\n",
    "combined_model.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68e97d8-0951-4d3a-900b-460f1f2e0ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training cGAN for 2 class generation\n",
    "\n",
    "\n",
    "epochs = 20000\n",
    "batch_size = 2\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "save_dir = 'YOUR_DIRECTORY_PATH'\n",
    "\n",
    "def save_images(epoch):\n",
    "    random_conditions = generate_random_conditions(num_classes)\n",
    "\n",
    "    \n",
    "    noise = np.random.normal(0, 1, (num_classes, 100))\n",
    "    generated_images = generator.predict([noise, random_conditions])\n",
    "\n",
    "    \n",
    "    fig, axs = plt.subplots(num_classes, 1, figsize=(8, 8))\n",
    "    for i in range(num_classes):\n",
    "        axs[i].imshow(generated_images[i])\n",
    "        axs[i].axis('off')\n",
    "\n",
    "    \n",
    "    filename = f'{save_dir}generated_images_epoch_{epoch}.png'\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "def generate_random_conditions(batch_size):\n",
    "    return np.random.randint(0, num_classes, (batch_size, 1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    idx = np.random.randint(0, new_dataset_train.shape[0], batch_size)\n",
    "    real_images = new_dataset_train[idx]\n",
    "    real_labels = new_tr_Y_train[idx]\n",
    "\n",
    "\n",
    "    noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "    generated_images = generator.predict([noise, real_labels])\n",
    "\n",
    "    discriminator.trainable = True\n",
    "    d_loss_real = discriminator.train_on_batch([real_images, real_labels], np.ones((batch_size, 1)))\n",
    "    d_loss_fake = discriminator.train_on_batch([generated_images, real_labels], np.zeros((batch_size, 1)))\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "    discriminator.trainable = False\n",
    "\n",
    "\n",
    "    random_conditions = generate_random_conditions(batch_size)\n",
    "    \n",
    "\n",
    "    g_loss = combined_model.train_on_batch([noise, random_conditions], np.ones((batch_size, 1)) )\n",
    "\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs}\")\n",
    "        print(f\"Discriminator Loss: {d_loss[0]}, Accuracy: {100 * d_loss[1]}%\")\n",
    "        print(f\"Generator Loss: {g_loss}\")\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        save_images(epoch)\n",
    "\n",
    "\n",
    "generator.save(\"generator_model.h5\")\n",
    "discriminator.save(\"discriminator_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ed6b7f-7309-4aba-b4e3-68956cef4d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the minority class\n",
    "\n",
    "num_images = # as per needed for each dataset to balance that.\n",
    "noise = np.random.normal(0, 1, (num_images, 100))\n",
    "new_images_1 = generator.predict([noise, np.ones(num_images)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b971f69-1b90-4891-baba-ae25d9caee9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using inverse transformation to get back tabular data\n",
    "inv_tran1 = IMGTR.inverse_transform(new_images_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39716dd-75e3-4a9b-9d4b-2499e77d801d",
   "metadata": {},
   "source": [
    "# CNN using Augmentation and CNN w/o Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153bdeb9-4e85-450e-97c0-cea28f9e6f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.layers import Dense, LeakyReLU\n",
    "\n",
    "\n",
    "new_labels = np.ones(new_images_1.shape[0])\n",
    "\n",
    "new_y_train = np.concatenate((new_tr_Y_train, new_labels), axis=0)\n",
    "\n",
    "new_X_train = np.concatenate((new_dataset_train, new_images_1), axis=0)\n",
    "\n",
    "auc_metric = tf.keras.metrics.AUC()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# Add fully connected layers\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy',  metrics=[tf.keras.metrics.AUC(from_logits=True)])\n",
    "\n",
    "# Train the model\n",
    "# Use this line for CNN w/o augmentation\n",
    "model.fit(new_dataset_train, new_tr_Y_train, epochs=20, batch_size=4,  validation_data=(new_dataset_test, new_tr_y_test))\n",
    "\n",
    "# Use this line for CNN w augmentation\n",
    "model.fit(new_X_train, new_y_train, epochs=20, batch_size=4,  validation_data=(new_dataset_test, new_tr_y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, auc = model.evaluate(new_dataset_test, new_tr_y_test)\n",
    "print(\"Test auc:\", auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086e6de6-c94b-4820-a810-fa84ce6b690d",
   "metadata": {},
   "source": [
    "# XGBoost using Augmentation and XGBoost w/o Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a34e1d-a059-4dec-a7fe-39e50f88d192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost w/o Augmentation\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Define your XGBoost model\n",
    "model = xgb.XGBClassifier(\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    objective='binary:logistic'\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(normalized_X_train, new_tr_Y_train)\n",
    "\n",
    "\n",
    "y_prob1 = model.predict_proba(normalized_X_test)[:, 1]\n",
    "\n",
    "\n",
    "roc_auc = roc_auc_score(new_tr_y_test, y_prob1)\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9256eb-44ec-4fe8-ba29-23e81d4d2eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost using Augmentation\n",
    "\n",
    "new_labels = np.ones(new_images_1.shape[0])\n",
    "\n",
    "new_y_train = np.concatenate((new_tr_Y_train, new_labels), axis=0)\n",
    "\n",
    "new_X_train = np.concatenate((normalized_X_train, inv_tran1)), axis=0)\n",
    "\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    objective='binary:logistic'\n",
    ")\n",
    "\n",
    "model.fit(new_X_train, new_y_train)\n",
    "\n",
    "\n",
    "y_prob2 = model.predict_proba(normalized_X_test)[:, 1]\n",
    "\n",
    "# Calculate the ROC AUC score\n",
    "roc_auc = roc_auc_score(new_tr_y_test, y_prob2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96569c7-cd92-4525-874c-2dffb43cc2da",
   "metadata": {},
   "source": [
    "# Three class generation using cGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a75933-50d3-45d1-bda5-1ee5b83f0be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this context, the architecture will remain consistent; however, the only variation will be in the num_classes.\n",
    "\n",
    "\n",
    "epochs = 20000\n",
    "batch_size = 2\n",
    "\n",
    "num_classes = 3\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "save_dir = 'YOUR_DIRECTORY_PATH'\n",
    "\n",
    "def save_images(epoch):\n",
    "    random_conditions = generate_random_conditions(num_classes)\n",
    "\n",
    "    \n",
    "    noise = np.random.normal(0, 1, (num_classes, 100))\n",
    "    generated_images = generator.predict([noise, random_conditions])\n",
    "\n",
    "    \n",
    "    fig, axs = plt.subplots(num_classes, 1, figsize=(8, 8))\n",
    "    for i in range(num_classes):\n",
    "        axs[i].imshow(generated_images[i])\n",
    "        axs[i].axis('off')\n",
    "\n",
    "    \n",
    "    filename = f'{save_dir}generated_images_epoch_{epoch}.png'\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "def generate_random_conditions(batch_size):\n",
    "    return np.random.randint(0, num_classes, (batch_size, 1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    idx = np.random.randint(0, new_dataset_train.shape[0], batch_size)\n",
    "    real_images = new_dataset_train[idx]\n",
    "    real_labels = new_tr_Y_train[idx]\n",
    "\n",
    "\n",
    "    noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "    generated_images = generator.predict([noise, real_labels])\n",
    "\n",
    "    discriminator.trainable = True\n",
    "    d_loss_real = discriminator.train_on_batch([real_images, real_labels], np.ones((batch_size, 1)))\n",
    "    d_loss_fake = discriminator.train_on_batch([generated_images, real_labels], np.zeros((batch_size, 1)))\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "    discriminator.trainable = False\n",
    "\n",
    "\n",
    "    random_conditions = generate_random_conditions(batch_size)\n",
    "    \n",
    "\n",
    "    g_loss = combined_model.train_on_batch([noise, random_conditions], np.ones((batch_size, 1)) )\n",
    "\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}/{epochs}\")\n",
    "        print(f\"Discriminator Loss: {d_loss[0]}, Accuracy: {100 * d_loss[1]}%\")\n",
    "        print(f\"Generator Loss: {g_loss}\")\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        save_images(epoch)\n",
    "\n",
    "\n",
    "generator.save(\"generator_model.h5\")\n",
    "discriminator.save(\"discriminator_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0433a667-128e-488b-b145-5494fc279171",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_imgs = # as per needed for each class to balance that.\n",
    "\n",
    "random_conditions_class_2 = np.full((2500, 1), 1)\n",
    "noise = np.random.normal(0, 1, (2500, 100))\n",
    "generated_images_class_2 = generator.predict([noise, random_conditions_class_2])\n",
    "\n",
    "\n",
    "random_conditions_class_3 = np.full((2500, 1), 2)\n",
    "noise = np.random.normal(0, 1, (2500, 100))\n",
    "generated_images_class_3 = generator.predict([noise, random_conditions_class_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b96cf03-93e3-4e04-84f3-8836fe250183",
   "metadata": {},
   "source": [
    "# ADASYN, SMOTE, and CTGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9efa95-9858-4766-a4fa-e022867affe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smote\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "smote = SMOTE()\n",
    "X_resampled, y_resampled = smote.fit_resample(normalized_X_train, new_tr_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fbfce5-0511-40ec-afc7-509235f5ec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Boosting using augmentation\n",
    "\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    objective='binary:logistic'\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(X_resampled, y_resampled)\n",
    "\n",
    "\n",
    "y_prob2 = model.predict_proba(normalized_X_test)[:, 1]\n",
    "\n",
    "# Calculate the ROC AUC score\n",
    "roc_auc = roc_auc_score(new_tr_y_test, y_prob2)\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08d8489-cda8-4691-a6a7-9f4168b149dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADASYN\n",
    "\n",
    "from imblearn.over_sampling import ADASYN\n",
    "adasyn = ADASYN(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = adasyn.fit_resample(normalized_X_train, new_tr_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc51379e-ec38-4abd-8d06-001ad0514175",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Boosting using augmentation\n",
    "\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    objective='binary:logistic'\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(X_resampled, y_resampled)\n",
    "\n",
    "\n",
    "y_prob2 = model.predict_proba(normalized_X_test)[:, 1]\n",
    "\n",
    "# Calculate the ROC AUC score\n",
    "roc_auc = roc_auc_score(new_tr_y_test, y_prob2)\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8712a0eb-2e2f-4a4e-9521-fbe3e76dae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctgan import CTGAN\n",
    "\n",
    "num_epochs = #\n",
    "\n",
    "# list of conditions (here the target variable)\n",
    "discrete_columns []\n",
    "\n",
    "ctgan = CTGAN(epochs=num_epochs)\n",
    "ctgan.fit(df, discrete_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c9baa5-a127-4dfd-b0e8-0f7f35458d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_column = '' # condition_column name\n",
    "condition_value = # the minority class\n",
    "num_samples = # number of samples needed to balance data\n",
    "\n",
    "synthetic_data = ctgan.sample(num_samples, condition_column=condition_column, condition_value=condition_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8548f8-40e0-4732-99a6-ead96f5e3d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = synthetic_data[synthetic_data[condition_column] == condition_value]\n",
    "new_tr_Xxx_train1 = filtered_data.drop(columns=[condition_column])\n",
    "new_tr_Yxx_train1 = filtered_data[condition_column] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e695b8d-1039-4fc4-ab18-04e22ab085a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Boosting using augmentation\n",
    "\n",
    "\n",
    "\n",
    "new_y_train = np.concatenate((new_tr_Y_train,new_tr_Yxx_train1), axis=0)\n",
    "\n",
    "new_X_train = np.concatenate((new_tr_X_train, new_tr_Xxx_train1), axis=0)\n",
    "\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    objective='binary:logistic'\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(new_X_train, new_y_train)\n",
    "\n",
    "y_prob2 = model.predict_proba(new_tr_X_test)[:, 1]\n",
    "\n",
    "# Calculate the ROC AUC score\n",
    "roc_auc = roc_auc_score(new_tr_y_test, y_prob2)\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
